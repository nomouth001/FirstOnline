#!/usr/bin/env python3
"""
ë°ì´í„°ë² ì´ìŠ¤ ë™ê¸°í™” ìœ í‹¸ë¦¬í‹°
ë¡œì»¬ SQLiteì™€ ì˜¨ë¼ì¸ PostgreSQL ê°„ì˜ ë°ì´í„° ë™ê¸°í™”ë¥¼ ë‹´ë‹¹í•©ë‹ˆë‹¤.
"""

import os
import sys
import json
import logging
from datetime import datetime
from typing import Dict, List, Optional, Any
import psycopg2
from psycopg2.extras import RealDictCursor
import sqlite3

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from app import app, db
from models import User, StockList, Stock, AnalysisHistory, NewsletterSubscription, EmailLog

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DatabaseSyncManager:
    """ë°ì´í„°ë² ì´ìŠ¤ ë™ê¸°í™” ê´€ë¦¬ì"""
    
    def __init__(self):
        self.local_db_path = 'app.db'
        self.postgres_url = os.getenv('DATABASE_URL', '').replace('postgres://', 'postgresql://')
        self.sync_tables = [
            'users', 'stock_lists', 'stocks', 'analysis_history', 
            'newsletter_subscriptions', 'email_logs'
        ]
        
    def export_data_to_json(self, target_env='local') -> Dict[str, Any]:
        """ë°ì´í„°ë² ì´ìŠ¤ ë°ì´í„°ë¥¼ JSONìœ¼ë¡œ ë‚´ë³´ë‚´ê¸°"""
        logger.info(f"ğŸ”„ {target_env} ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë°ì´í„° ë‚´ë³´ë‚´ê¸° ì‹œì‘")
        
        data = {}
        
        with app.app_context():
            try:
                # ì‚¬ìš©ì ë°ì´í„°
                users = User.query.all()
                data['users'] = [{
                    'id': user.id,
                    'username': user.username,
                    'email': user.email,
                    'password_hash': user.password_hash,
                    'first_name': user.first_name,
                    'last_name': user.last_name,
                    'is_active': user.is_active,
                    'is_verified': user.is_verified,
                    'is_admin': user.is_admin,
                    'is_withdrawn': user.is_withdrawn,
                    'withdrawn_at': user.withdrawn_at.isoformat() if user.withdrawn_at else None,
                    'created_at': user.created_at.isoformat() if user.created_at else None,
                    'updated_at': user.updated_at.isoformat() if user.updated_at else None
                } for user in users]
                
                # ì£¼ì‹ ë¦¬ìŠ¤íŠ¸ ë°ì´í„°
                stock_lists = StockList.query.all()
                data['stock_lists'] = [{
                    'id': sl.id,
                    'name': sl.name,
                    'description': sl.description,
                    'is_public': sl.is_public,
                    'is_default': sl.is_default,
                    'user_id': sl.user_id,
                    'created_at': sl.created_at.isoformat() if sl.created_at else None,
                    'updated_at': sl.updated_at.isoformat() if sl.updated_at else None
                } for sl in stock_lists]
                
                # ì£¼ì‹ ë°ì´í„°
                stocks = Stock.query.all()
                data['stocks'] = [{
                    'id': stock.id,
                    'ticker': stock.ticker,
                    'name': stock.name,
                    'stock_list_id': stock.stock_list_id,
                    'added_at': stock.added_at.isoformat() if stock.added_at else None
                } for stock in stocks]
                
                # ë¶„ì„ íˆìŠ¤í† ë¦¬ ë°ì´í„°
                analysis_history = AnalysisHistory.query.all()
                data['analysis_history'] = [{
                    'id': ah.id,
                    'ticker': ah.ticker,
                    'analysis_date': ah.analysis_date.isoformat() if ah.analysis_date else None,
                    'analysis_type': ah.analysis_type,
                    'chart_path': ah.chart_path,
                    'analysis_path': ah.analysis_path,
                    'summary': ah.summary,
                    'status': ah.status,
                    'user_id': ah.user_id,
                    'created_at': ah.created_at.isoformat() if ah.created_at else None
                } for ah in analysis_history]
                
                # ë‰´ìŠ¤ë ˆí„° êµ¬ë… ë°ì´í„°
                newsletter_subs = NewsletterSubscription.query.all()
                data['newsletter_subscriptions'] = [{
                    'id': ns.id,
                    'user_id': ns.user_id,
                    'is_active': ns.is_active,
                    'frequency': ns.frequency,
                    'send_time': ns.send_time.isoformat() if ns.send_time else None,
                    'include_charts': ns.include_charts,
                    'include_summary': ns.include_summary,
                    'include_technical_analysis': ns.include_technical_analysis,
                    'unsubscribe_token': ns.unsubscribe_token,
                    'created_at': ns.created_at.isoformat() if ns.created_at else None,
                    'updated_at': ns.updated_at.isoformat() if ns.updated_at else None
                } for ns in newsletter_subs]
                
                # ì´ë©”ì¼ ë¡œê·¸ ë°ì´í„°
                email_logs = EmailLog.query.all()
                data['email_logs'] = [{
                    'id': el.id,
                    'user_id': el.user_id,
                    'email_type': el.email_type,
                    'subject': el.subject,
                    'status': el.status,
                    'sent_at': el.sent_at.isoformat() if el.sent_at else None,
                    'error_message': el.error_message
                } for el in email_logs]
                
                logger.info(f"âœ… {target_env} ë°ì´í„° ë‚´ë³´ë‚´ê¸° ì™„ë£Œ")
                logger.info(f"ğŸ“Š ë‚´ë³´ë‚¸ ë°ì´í„° í†µê³„:")
                for table, items in data.items():
                    logger.info(f"  - {table}: {len(items)}ê°œ")
                
                return data
                
            except Exception as e:
                logger.error(f"âŒ ë°ì´í„° ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {e}")
                raise
    
    def import_data_from_json(self, data: Dict[str, Any], target_env='local'):
        """JSON ë°ì´í„°ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ê°€ì ¸ì˜¤ê¸°"""
        logger.info(f"ğŸ”„ {target_env} ë°ì´í„°ë² ì´ìŠ¤ë¡œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° ì‹œì‘")
        
        with app.app_context():
            try:
                # ê¸°ì¡´ ë°ì´í„° ë°±ì—… (í•„ìš”ì‹œ)
                backup_data = self.export_data_to_json(target_env)
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                backup_file = f"backup_{target_env}_{timestamp}.json"
                
                with open(backup_file, 'w', encoding='utf-8') as f:
                    json.dump(backup_data, f, indent=2, ensure_ascii=False)
                logger.info(f"ğŸ“‹ ê¸°ì¡´ ë°ì´í„° ë°±ì—… ì™„ë£Œ: {backup_file}")
                
                # ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (ì°¸ì¡° ë¬´ê²°ì„± ìˆœì„œ ê³ ë ¤)
                
                # 1. ì‚¬ìš©ì ë°ì´í„° ë¨¼ì € ê°€ì ¸ì˜¤ê¸°
                if 'users' in data:
                    User.query.delete()
                    for user_data in data['users']:
                        user = User(
                            id=user_data['id'],
                            username=user_data['username'],
                            email=user_data['email'],
                            password_hash=user_data['password_hash'],
                            first_name=user_data.get('first_name'),
                            last_name=user_data.get('last_name'),
                            is_active=user_data.get('is_active', True),
                            is_verified=user_data.get('is_verified', False),
                            is_admin=user_data.get('is_admin', False),
                            is_withdrawn=user_data.get('is_withdrawn', False),
                            withdrawn_at=datetime.fromisoformat(user_data['withdrawn_at']) if user_data.get('withdrawn_at') else None,
                            created_at=datetime.fromisoformat(user_data['created_at']) if user_data.get('created_at') else None,
                            updated_at=datetime.fromisoformat(user_data['updated_at']) if user_data.get('updated_at') else None
                        )
                        db.session.add(user)
                
                # 2. ì£¼ì‹ ë¦¬ìŠ¤íŠ¸ ë°ì´í„°
                if 'stock_lists' in data:
                    StockList.query.delete()
                    for sl_data in data['stock_lists']:
                        stock_list = StockList(
                            id=sl_data['id'],
                            name=sl_data['name'],
                            description=sl_data.get('description'),
                            is_public=sl_data.get('is_public', False),
                            is_default=sl_data.get('is_default', False),
                            user_id=sl_data['user_id'],
                            created_at=datetime.fromisoformat(sl_data['created_at']) if sl_data.get('created_at') else None,
                            updated_at=datetime.fromisoformat(sl_data['updated_at']) if sl_data.get('updated_at') else None
                        )
                        db.session.add(stock_list)
                
                # 3. ì£¼ì‹ ë°ì´í„°
                if 'stocks' in data:
                    Stock.query.delete()
                    for stock_data in data['stocks']:
                        stock = Stock(
                            id=stock_data['id'],
                            ticker=stock_data['ticker'],
                            name=stock_data.get('name'),
                            stock_list_id=stock_data['stock_list_id'],
                            added_at=datetime.fromisoformat(stock_data['added_at']) if stock_data.get('added_at') else None
                        )
                        db.session.add(stock)
                
                # 4. ë¶„ì„ íˆìŠ¤í† ë¦¬ ë°ì´í„°
                if 'analysis_history' in data:
                    AnalysisHistory.query.delete()
                    for ah_data in data['analysis_history']:
                        analysis_history = AnalysisHistory(
                            id=ah_data['id'],
                            ticker=ah_data['ticker'],
                            analysis_date=datetime.fromisoformat(ah_data['analysis_date']).date() if ah_data.get('analysis_date') else None,
                            analysis_type=ah_data.get('analysis_type', 'daily'),
                            chart_path=ah_data.get('chart_path'),
                            analysis_path=ah_data.get('analysis_path'),
                            summary=ah_data.get('summary'),
                            status=ah_data.get('status', 'completed'),
                            user_id=ah_data['user_id'],
                            created_at=datetime.fromisoformat(ah_data['created_at']) if ah_data.get('created_at') else None
                        )
                        db.session.add(analysis_history)
                
                # 5. ë‰´ìŠ¤ë ˆí„° êµ¬ë… ë°ì´í„°
                if 'newsletter_subscriptions' in data:
                    NewsletterSubscription.query.delete()
                    for ns_data in data['newsletter_subscriptions']:
                        # send_time í•„ë“œ ì²˜ë¦¬: "09:00:00" í˜•ì‹ì˜ ì‹œê°„ ë¬¸ìì—´
                        send_time = None
                        if ns_data.get('send_time'):
                            try:
                                # ë‹¨ìˆœ ì‹œê°„ í˜•ì‹ "HH:MM:SS"
                                send_time = datetime.strptime(ns_data['send_time'], '%H:%M:%S').time()
                            except ValueError:
                                try:
                                    # ISO í˜•ì‹ìœ¼ë¡œ ì‹œë„
                                    send_time = datetime.fromisoformat(ns_data['send_time']).time()
                                except ValueError:
                                    logger.warning(f"ì‹œê°„ í˜•ì‹ ì˜¤ë¥˜: {ns_data['send_time']}")
                                    send_time = None
                        
                        newsletter_sub = NewsletterSubscription(
                            id=ns_data['id'],
                            user_id=ns_data['user_id'],
                            is_active=ns_data.get('is_active', True),
                            frequency=ns_data.get('frequency', 'daily'),
                            send_time=send_time,
                            include_charts=ns_data.get('include_charts', True),
                            include_summary=ns_data.get('include_summary', True),
                            include_technical_analysis=ns_data.get('include_technical_analysis', True),
                            unsubscribe_token=ns_data.get('unsubscribe_token'),
                            created_at=datetime.fromisoformat(ns_data['created_at']) if ns_data.get('created_at') else None,
                            updated_at=datetime.fromisoformat(ns_data['updated_at']) if ns_data.get('updated_at') else None
                        )
                        db.session.add(newsletter_sub)
                
                # 6. ì´ë©”ì¼ ë¡œê·¸ ë°ì´í„°
                if 'email_logs' in data:
                    EmailLog.query.delete()
                    for el_data in data['email_logs']:
                        email_log = EmailLog(
                            id=el_data['id'],
                            user_id=el_data['user_id'],
                            email_type=el_data['email_type'],
                            subject=el_data['subject'],
                            status=el_data.get('status', 'sent'),
                            sent_at=datetime.fromisoformat(el_data['sent_at']) if el_data.get('sent_at') else None,
                            error_message=el_data.get('error_message')
                        )
                        db.session.add(email_log)
                
                db.session.commit()
                logger.info(f"âœ… {target_env} ë°ì´í„° ê°€ì ¸ì˜¤ê¸° ì™„ë£Œ")
                
            except Exception as e:
                db.session.rollback()
                logger.error(f"âŒ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
                raise
    
    def sync_local_to_remote(self):
        """ë¡œì»¬ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì˜¨ë¼ì¸ìœ¼ë¡œ ë™ê¸°í™”"""
        logger.info("ğŸ”„ ë¡œì»¬ â†’ ì˜¨ë¼ì¸ ë™ê¸°í™” ì‹œì‘")
        
        # ë¡œì»¬ ë°ì´í„° ë‚´ë³´ë‚´ê¸°
        local_data = self.export_data_to_json('local')
        
        # ì˜¨ë¼ì¸ í™˜ê²½ìœ¼ë¡œ ì „í™˜
        original_db_uri = app.config['SQLALCHEMY_DATABASE_URI']
        if self.postgres_url:
            app.config['SQLALCHEMY_DATABASE_URI'] = self.postgres_url
            db.init_app(app)
            
            # ì˜¨ë¼ì¸ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            self.import_data_from_json(local_data, 'remote')
            
            # ì›ë˜ ì„¤ì • ë³µì›
            app.config['SQLALCHEMY_DATABASE_URI'] = original_db_uri
            db.init_app(app)
            
            logger.info("âœ… ë¡œì»¬ â†’ ì˜¨ë¼ì¸ ë™ê¸°í™” ì™„ë£Œ")
        else:
            logger.error("âŒ DATABASE_URL í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ")
    
    def sync_remote_to_local(self):
        """ì˜¨ë¼ì¸ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ë¡œì»¬ë¡œ ë™ê¸°í™”"""
        logger.info("ğŸ”„ ì˜¨ë¼ì¸ â†’ ë¡œì»¬ ë™ê¸°í™” ì‹œì‘")
        
        if not self.postgres_url:
            logger.error("âŒ DATABASE_URL í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ")
            return
        
        # ì˜¨ë¼ì¸ í™˜ê²½ìœ¼ë¡œ ì „í™˜
        original_db_uri = app.config['SQLALCHEMY_DATABASE_URI']
        app.config['SQLALCHEMY_DATABASE_URI'] = self.postgres_url
        db.init_app(app)
        
        # ì˜¨ë¼ì¸ ë°ì´í„° ë‚´ë³´ë‚´ê¸°
        remote_data = self.export_data_to_json('remote')
        
        # ë¡œì»¬ í™˜ê²½ìœ¼ë¡œ ë³µì›
        app.config['SQLALCHEMY_DATABASE_URI'] = original_db_uri
        db.init_app(app)
        
        # ë¡œì»¬ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        self.import_data_from_json(remote_data, 'local')
        
        logger.info("âœ… ì˜¨ë¼ì¸ â†’ ë¡œì»¬ ë™ê¸°í™” ì™„ë£Œ")
    
    def compare_databases(self):
        """ë‘ ë°ì´í„°ë² ì´ìŠ¤ ë¹„êµ"""
        logger.info("ğŸ” ë°ì´í„°ë² ì´ìŠ¤ ë¹„êµ ì‹œì‘")
        
        # ë¡œì»¬ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        local_data = self.export_data_to_json('local')
        
        # ì˜¨ë¼ì¸ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        if self.postgres_url:
            original_db_uri = app.config['SQLALCHEMY_DATABASE_URI']
            app.config['SQLALCHEMY_DATABASE_URI'] = self.postgres_url
            db.init_app(app)
            
            remote_data = self.export_data_to_json('remote')
            
            app.config['SQLALCHEMY_DATABASE_URI'] = original_db_uri
            db.init_app(app)
            
            # ë¹„êµ ê²°ê³¼ ì¶œë ¥
            print("\nğŸ“Š ë°ì´í„°ë² ì´ìŠ¤ ë¹„êµ ê²°ê³¼:")
            print("=" * 50)
            for table in self.sync_tables:
                local_count = len(local_data.get(table, []))
                remote_count = len(remote_data.get(table, []))
                status = "âœ… ë™ì¼" if local_count == remote_count else "âš ï¸ ì°¨ì´"
                print(f"{table:25} | ë¡œì»¬: {local_count:3d} | ì˜¨ë¼ì¸: {remote_count:3d} | {status}")
        else:
            logger.error("âŒ DATABASE_URL í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description='ë°ì´í„°ë² ì´ìŠ¤ ë™ê¸°í™” ë„êµ¬')
    parser.add_argument('action', choices=['export', 'import', 'sync-to-remote', 'sync-to-local', 'compare'],
                       help='ìˆ˜í–‰í•  ë™ì‘')
    parser.add_argument('--file', help='ê°€ì ¸ì˜¤ê¸°/ë‚´ë³´ë‚´ê¸° íŒŒì¼ ê²½ë¡œ')
    
    args = parser.parse_args()
    
    sync_manager = DatabaseSyncManager()
    
    try:
        if args.action == 'export':
            data = sync_manager.export_data_to_json()
            file_path = args.file or f"db_export_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
            print(f"âœ… ë°ì´í„° ë‚´ë³´ë‚´ê¸° ì™„ë£Œ: {file_path}")
        
        elif args.action == 'import':
            if not args.file:
                print("âŒ --file ì˜µì…˜ì´ í•„ìš”í•©ë‹ˆë‹¤")
                return
            
            with open(args.file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            sync_manager.import_data_from_json(data)
            print("âœ… ë°ì´í„° ê°€ì ¸ì˜¤ê¸° ì™„ë£Œ")
        
        elif args.action == 'sync-to-remote':
            sync_manager.sync_local_to_remote()
        
        elif args.action == 'sync-to-local':
            sync_manager.sync_remote_to_local()
        
        elif args.action == 'compare':
            sync_manager.compare_databases()
    
    except Exception as e:
        logger.error(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main() 